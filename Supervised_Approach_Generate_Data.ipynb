{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UN-GCPDS/Unemployment-Rate-Prediction/blob/main/Supervised_Approach_Generate_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxJaI2hOYFNP"
      },
      "source": [
        "#Funciones y librerías"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5MEWDu12jEa",
        "outputId": "e14d0a68-3eb4-4a0f-8ed2-0ec4fd3f1d78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/129.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m122.9/129.1 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# 1. Instalación de librerías externas\n",
        "# ========================================\n",
        "!pip install pmdarima==1.8.5\n",
        "!pip install -q -U keras-tuner\n",
        "\n",
        "# ========================================\n",
        "# 2. Librerías base y utilidades generales\n",
        "# ========================================\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "\n",
        "# ========================================\n",
        "# 3. Visualización\n",
        "# ========================================\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# ========================================\n",
        "# 4. Preprocesamiento de datos\n",
        "# ========================================\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# ========================================\n",
        "# 5. Modelos de regresión clásicos\n",
        "# ========================================\n",
        "from sklearn.linear_model import LinearRegression, Lasso, ElasticNet, SGDRegressor, BayesianRidge\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.kernel_ridge import KernelRidge\n",
        "\n",
        "# ========================================\n",
        "# 6. Modelos bayesianos y de procesos gaussianos\n",
        "# ========================================\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF, ConstantKernel, WhiteKernel, Matern, ExpSineSquared, DotProduct\n",
        "\n",
        "# ========================================\n",
        "# 7. Modelos estadísticos y series temporales\n",
        "# ========================================\n",
        "import pmdarima as pm\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
        "\n",
        "# ========================================\n",
        "# 8. Evaluación de modelos\n",
        "# ========================================\n",
        "from sklearn.metrics import make_scorer, mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# ========================================\n",
        "# 9. Keras Tuner para ajuste de hiperparámetros\n",
        "# ========================================\n",
        "from keras_tuner import BayesianOptimization as OriginalBayesianOptimization\n",
        "from keras_tuner.oracles import BayesianOptimizationOracle as OriginalBayesianOptimizationOracle\n",
        "from keras_tuner import Tuner, Objective\n",
        "from keras_tuner.tuners import SklearnTuner\n",
        "from keras_tuner.src.engine import hyperparameters as hp_module\n",
        "from keras_tuner.src.engine import oracle as oracle_module\n",
        "from keras_tuner.src.engine import trial as trial_module\n",
        "from keras_tuner.src.engine import tuner as tuner_module\n",
        "from keras_tuner.src.api_export import keras_tuner_export\n",
        "\n",
        "# ========================================\n",
        "# 10. Configuración de warnings\n",
        "# ========================================\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Non-stationary starting autoregressive parameters found\")\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Non-invertible starting MA parameters found\")\n",
        "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning, message=\"Maximum Likelihood optimization failed to converge\")\n",
        "\n",
        "class CustomBayesianOptimizationOracle(OriginalBayesianOptimizationOracle):\n",
        "    def __init__(self, *args, nu=0.5, **kwargs):\n",
        "        self.nu = nu  # Establecer nu antes de llamar al constructor de la clase base\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.gpr = self._make_gpr()  # Asegurarse de que _make_gpr se llame después de establecer nu\n",
        "    def _gpr_trained(self):\n",
        "        if self.gpr is None:\n",
        "            print(\"There is no GPR model trained yet.\")\n",
        "            return None\n",
        "        else:\n",
        "            return self.gpr\n",
        "    def _make_gpr(self):\n",
        "        return sklearn.gaussian_process.GaussianProcessRegressor(\n",
        "            kernel=sklearn.gaussian_process.kernels.Matern(nu=self.nu),\n",
        "            n_restarts_optimizer=20,\n",
        "            normalize_y=True,\n",
        "            alpha=self.alpha,\n",
        "            random_state=self.seed,\n",
        "        )\n",
        "\n",
        "class CustomBayesianOptimization(Tuner):\n",
        "    def __init__(\n",
        "        self,\n",
        "        hypermodel=None,\n",
        "        objective=None,\n",
        "        max_trials=10,\n",
        "        num_initial_points=None,\n",
        "        alpha=1e-4,\n",
        "        beta=2.6,\n",
        "        seed=None,\n",
        "        hyperparameters=None,\n",
        "        tune_new_entries=True,\n",
        "        allow_new_entries=True,\n",
        "        max_retries_per_trial=0,\n",
        "        max_consecutive_failed_trials=3,\n",
        "        nu=0.5,  # Valor por defecto para nu\n",
        "        **kwargs\n",
        "    ):\n",
        "        oracle = CustomBayesianOptimizationOracle(\n",
        "            objective=objective,\n",
        "            max_trials=max_trials,\n",
        "            num_initial_points=num_initial_points,\n",
        "            alpha=alpha,\n",
        "            beta=beta,\n",
        "            seed=seed,\n",
        "            hyperparameters=hyperparameters,\n",
        "            tune_new_entries=tune_new_entries,\n",
        "            allow_new_entries=allow_new_entries,\n",
        "            max_retries_per_trial=max_retries_per_trial,\n",
        "            max_consecutive_failed_trials=max_consecutive_failed_trials,\n",
        "            nu=nu  # Pasar el parámetro nu\n",
        "        )\n",
        "        # Llama directamente al constructor de la clase base Tuner\n",
        "        super().__init__(oracle=oracle, hypermodel=hypermodel, **kwargs)\n",
        "\n",
        "class CustomSklearnBayesianOptimization(SklearnTuner):\n",
        "    def __init__(\n",
        "        self,\n",
        "        hypermodel=None,\n",
        "        objective=None,\n",
        "        max_trials=10,\n",
        "        num_initial_points=None,\n",
        "        alpha=1e-1,\n",
        "        beta=2.6,\n",
        "        seed=None,\n",
        "        hyperparameters=None,\n",
        "        tune_new_entries=True,\n",
        "        allow_new_entries=True,\n",
        "        max_retries_per_trial=0,\n",
        "        max_consecutive_failed_trials=3,\n",
        "        nu=0.5,  # Valor por defecto para nu\n",
        "        scoring=None,\n",
        "        cv=None,\n",
        "        **kwargs\n",
        "    ):\n",
        "        oracle = CustomBayesianOptimizationOracle(\n",
        "            objective=objective,\n",
        "            max_trials=max_trials,\n",
        "            num_initial_points=num_initial_points,\n",
        "            alpha=alpha,\n",
        "            beta=beta,\n",
        "            seed=seed,\n",
        "            hyperparameters=hyperparameters,\n",
        "            tune_new_entries=tune_new_entries,\n",
        "            allow_new_entries=allow_new_entries,\n",
        "            max_retries_per_trial=max_retries_per_trial,\n",
        "            max_consecutive_failed_trials=max_consecutive_failed_trials,\n",
        "            nu=nu  # Pasar el parámetro nu\n",
        "        )\n",
        "        # Llama directamente al constructor de la clase base SklearnTuner\n",
        "        super().__init__(oracle=oracle, hypermodel=hypermodel, scoring=scoring, cv=cv, **kwargs)\n",
        "class TimeSeriesPipeline:\n",
        "    def __init__(self, model_type='SARIMA', seasonal=True, scaler=None, m=12):\n",
        "        self.model_type = model_type\n",
        "        self.scaler = scaler\n",
        "        self.model = None\n",
        "        self.seasonal = seasonal\n",
        "        self.m = m  # Número de períodos en una temporada para SARIMA\n",
        "    def fit(self, y_train):\n",
        "        # Aplicar escalado si se proporciona\n",
        "        if self.scaler:\n",
        "            y_train = self.scaler.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
        "        # Utilizar auto_arima para encontrar los mejores parámetros\n",
        "        model_auto = auto_arima(\n",
        "            y_train,\n",
        "            seasonal=self.seasonal,\n",
        "            m=self.m,\n",
        "            trace=False,\n",
        "            error_action='ignore',\n",
        "            suppress_warnings=True,\n",
        "            stepwise=True\n",
        "        )\n",
        "        try:\n",
        "            order = model_auto.order\n",
        "        except:\n",
        "            model_auto = auto_arima(\n",
        "            y_train,\n",
        "            seasonal=self.seasonal,\n",
        "            m=1,\n",
        "            trace=False,\n",
        "            error_action='ignore',\n",
        "            suppress_warnings=True,\n",
        "            stepwise=True)\n",
        "        order = model_auto.order\n",
        "        seasonal_order = model_auto.seasonal_order if self.seasonal else (0, 0, 0, 0)\n",
        "        # Ajustar el modelo basado en el tipo especificado\n",
        "        if self.model_type == 'ARIMA':\n",
        "            self.model = ARIMA(y_train, order=order).fit()\n",
        "        elif self.model_type == 'SARIMA':\n",
        "            self.model = SARIMAX(y_train, order=order, seasonal_order=seasonal_order).fit()\n",
        "        # Guardar la información del modelo ajustado\n",
        "        self.model_info = {\n",
        "            'order': order,\n",
        "            'seasonal_order': seasonal_order,\n",
        "            'aic': self.model.aic,\n",
        "            'bic': self.model.bic\n",
        "        }\n",
        "    def predict(self, start, end):\n",
        "        # Realizar predicciones\n",
        "        y_pred = self.model.predict(start=start, end=end)\n",
        "        # Invertir escalado si se proporcionó\n",
        "        if self.scaler:\n",
        "            y_pred = self.scaler.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n",
        "        return y_pred\n",
        "# Clase TimeSeriesPipeline ya definida aquí...\n",
        "def build_pipeline(hp, y_train, model_type='KRRBF', seasonal_order=(1, 1, 1, 4), X=X):\n",
        "    # Estándarización\n",
        "    scaler = MinMaxScaler()\n",
        "\n",
        "    # Definición del modelo basado en el tipo\n",
        "    if model_type == \"KRRBF\":\n",
        "        model = KernelRidge(\n",
        "            kernel='rbf',\n",
        "            gamma=hp.Float(\"gamma\", 1e-3, 100, default=1, sampling=\"log\"),\n",
        "            alpha=hp.Float(\"alpha\", 1e-3, 100, default=1, sampling=\"log\")\n",
        "        )\n",
        "    elif model_type == \"KNN\":\n",
        "        model = KNeighborsRegressor(\n",
        "            n_neighbors=hp.Int(\"n_neighbors\", 1, 9, default=3)\n",
        "        )\n",
        "    elif model_type == \"GP\":\n",
        "        model = GaussianProcessRegressor(\n",
        "            alpha=hp.Float(\"alpha\", 1e-5, 1e2, default=1, sampling=\"log\"),\n",
        "            kernel=RBF(length_scale=np.ones(X.shape[1]),length_scale_bounds=(1,1000))#WhiteKernel()+Matern(,nu=0.5),normalize_y=True,\n",
        "        )\n",
        "    elif model_type == \"Linear\":\n",
        "        model = LinearRegression()\n",
        "    elif model_type == \"Lasso\":\n",
        "        model = Lasso(\n",
        "            alpha=hp.Float(\"alpha\", 1e-3, 1, default=1, sampling=\"log\")\n",
        "        )\n",
        "    elif model_type == \"Elastic\":\n",
        "        model = ElasticNet(\n",
        "            alpha=hp.Float(\"alpha\", 0.1, 1, sampling=\"log\"),\n",
        "            l1_ratio=hp.Float(\"l1_ratio\", 0.1, 1,sampling=\"log\")\n",
        "        )\n",
        "    elif model_type == \"SGD\":\n",
        "        model = SGDRegressor(\n",
        "            alpha=hp.Float(\"alpha\", 1e-3, 1, default=0.01, sampling=\"log\"),\n",
        "            l1_ratio=hp.Float(\"l1_ratio\", 0, 1, default=0.15)\n",
        "        )\n",
        "    elif model_type == \"BayesRid\":\n",
        "        model = BayesianRidge(\n",
        "            alpha_1=hp.Float(\"alpha_1\", 1e-6, 1, default=1e-6, sampling=\"log\"),\n",
        "            alpha_2=hp.Float(\"alpha_2\", 1e-6, 1, default=1e-6, sampling=\"log\"),\n",
        "            lambda_1=hp.Float(\"lambda_1\", 1e-6, 1, default=1e-6, sampling=\"log\"),\n",
        "            lambda_2=hp.Float(\"lambda_2\", 1e-6, 1, default=1e-6, sampling=\"log\")\n",
        "        )\n",
        "    elif model_type == \"RF\":\n",
        "        model = RandomForestRegressor(\n",
        "            max_depth=hp.Int(\"max_depth\", 1, 100, step=1, default=30),\n",
        "            n_estimators=hp.Int(\"n_estimators\", 1, 100, step=1, default=70)\n",
        "        )\n",
        "    elif model_type == \"SVR\":\n",
        "        model = SVR(\n",
        "            kernel=hp.Choice(\"kernel\", ['linear', 'poly', 'rbf', 'sigmoid'], default='rbf'),\n",
        "            C=hp.Float(\"C\", 0.1, 1, default=1),\n",
        "            epsilon=hp.Float(\"epsilon\", 1e-3, 0.1, default=0.1, sampling=\"log\")\n",
        "        )\n",
        "    elif model_type in ['ARIMA', 'SARIMA']:\n",
        "        # Crear una instancia de TimeSeriesPipeline para ARIMA o SARIMA\n",
        "        seasonal = (model_type == 'SARIMA')\n",
        "        ts_pipeline = TimeSeriesPipeline(\n",
        "            model_type=model_type,\n",
        "            seasonal=seasonal,\n",
        "            scaler=scaler,\n",
        "            m=seasonal_order[3] if seasonal else 1\n",
        "        )\n",
        "        # Ajustar el modelo con los datos de entrenamiento\n",
        "        ts_pipeline.fit(y_train)\n",
        "        # Retornar el modelo ajustado y la información del modelo\n",
        "        fitted_model = ts_pipeline\n",
        "        model_info = ts_pipeline.model_info\n",
        "        return fitted_model, model_info\n",
        "    else:\n",
        "        raise ValueError(\"Unrecognized model_type\")\n",
        "    # Creación del pipeline para los modelos de ML estándar\n",
        "    pipeline =Pipeline([(\"reg\", model)]) #Pipeline([('sca',scaler),(\"reg\", model)])\n",
        "    return pipeline\n",
        "class ModelBuilder:\n",
        "    def __init__(self, model_type, y_train, seasonal_order=(1, 1, 1, 3)):\n",
        "        self.model_type = model_type\n",
        "        self.seasonal_order = seasonal_order\n",
        "        self.y_train = y_train\n",
        "    def build_model(self, hp):\n",
        "        return build_pipeline(hp, self.y_train, self.model_type, self.seasonal_order)\n",
        "path='/content/drive/MyDrive/Doctorado/Optimización/'\n",
        "def time_series_cv(X, y, model_type='KRRBF', s=0, Na='', seasonal_order=(1, 1, 1, 1),train_years = 1,test_months = 3,step_months = 3 ):\n",
        "    tuners = []\n",
        "    Cont = 0\n",
        "    y_true_list = []\n",
        "    y_pred_list = []\n",
        "    y_std_list = []\n",
        "    best_params_list = []\n",
        "      # Años para entrenamiento\n",
        "      # Meses para predicción\n",
        "     # Meses para mover hacia adelante\n",
        "    for ind in range(12, X.shape[0] - test_months + 1, step_months):\n",
        "        # Definir los intervalos de entrenamiento y prueba\n",
        "        X_train, y_train = X[(ind - train_years * 12)*s:ind], y[(ind - train_years * 12)*s:ind]\n",
        "        X_test, y_test = X[ind:ind + test_months], y[ind:ind + test_months]\n",
        "        if model_type in ['ARIMA', 'SARIMA']:\n",
        "            model,best_params = ModelBuilder(model_type, y_train, seasonal_order).build_model(2)\n",
        "            y_pred = model.predict(start=len(y_train), end=len(y_train) + len(y_test) - 1)\n",
        "        else:\n",
        "            model_builder = ModelBuilder(model_type, y_train, seasonal_order)\n",
        "            tuner = CustomSklearnBayesianOptimization(\n",
        "                hypermodel=model_builder.build_model,\n",
        "                objective=Objective('score', direction='max'),\n",
        "                max_trials=15,\n",
        "                scoring=make_scorer(r2_score),\n",
        "                directory='.',\n",
        "                cv=None,\n",
        "                project_name=f'12{Na}CV{Cont}_{model_type}'\n",
        "            )\n",
        "            print(Cont)\n",
        "            Cont += 1\n",
        "            tuner.search(X_train, y_train)\n",
        "            tuners.append(tuner)\n",
        "            # Obtener y evaluar el mejor modelo de este fold\n",
        "            best_model = tuner.get_best_models(num_models=1)[0]\n",
        "            best_params = tuner.get_best_hyperparameters()[0].values\n",
        "            if model_type == 'GP':\n",
        "                kernel_params = best_model.get_params()['reg'].kernel_.get_params()\n",
        "                best_params.update(kernel_params)\n",
        "                y_pred, y_std = best_model.predict(X_test, return_std=True)\n",
        "                y_std_list.extend(y_std)\n",
        "            elif model_type in  ['Lasso','Elastic']:\n",
        "                # Extraer coeficientes de ElasticNet\n",
        "                final_model = best_model.named_steps['reg']\n",
        "                coef_params = final_model.coef_\n",
        "                best_params.update({'coef_' + str(i): coef for i, coef in enumerate(coef_params)})\n",
        "                y_pred = best_model.predict(X_test)\n",
        "            elif model_type == 'RF':\n",
        "                final_model = best_model.named_steps['reg']\n",
        "                importances = final_model.feature_importances_\n",
        "                best_params.update({'feature_importance_' + str(i): imp for i, imp in enumerate(importances)})\n",
        "                y_pred = best_model.predict(X_test)\n",
        "            else:\n",
        "                y_pred = best_model.predict(X_test)\n",
        "        best_params_list.append(best_params)\n",
        "        y_true_list.extend(y_test)\n",
        "        y_pred_list.extend(y_pred)\n",
        "    # Guardar y_true, y_pred y y_std (si existe) en un archivo Excel\n",
        "    results_dict = {'y_true': y_true_list, 'y_pred': y_pred_list}\n",
        "    if y_std_list:\n",
        "        results_dict['y_std'] = y_std_list\n",
        "    y_results = pd.DataFrame(results_dict)\n",
        "    y_results_file_path = f'{path}{test_months} Meses/Results/{model_type}_{s}.xlsx'\n",
        "    y_results.to_excel(y_results_file_path, index=False)\n",
        "    # Guardar los mejores hiperparámetros (incluyendo kernel si aplica) en un archivo Excel\n",
        "    best_params_df = pd.DataFrame(best_params_list)\n",
        "    best_params_file_path = f'{path}{test_months} Meses/Params/{model_type}_{s}.xlsx'\n",
        "    best_params_df.to_excel(best_params_file_path, index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGGIGYweOG-N"
      },
      "source": [
        "#Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsviLDjF2Ply"
      },
      "outputs": [],
      "source": [
        "FILEID =\"1BidLKcEXF6h-LKKWixtKmQ_g5hgvrkgd\"\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id='$FILEID -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=\"$FILEID -O datos.xlsx && rm -rf /tmp/cookies.txt\n",
        "!dir\n",
        "\n",
        "import pandas as pd\n",
        "Xdata = pd.read_excel('datos.xlsx')\n",
        "Xdata.head()\n",
        "Xdata.fillna(Xdata.mean(), inplace=True)\n",
        "Xdata.describe()\n",
        "\n",
        "\n",
        "\n",
        "Xdata=Xdata[['Año', 'Mes', 'TGP',  'salarios reales', 'Indicador de Seguimiento a la Economía',\n",
        "       'imporenta real', 'iva real',\n",
        "       'Tasa de intervención de política monetaria (%)','TD']]#'TD-1','TD-2','TD-3' ,'TD']]\n",
        "Xdata.rename(columns={'Año': 'Year', 'Mes':'Month', 'TGP':'GPR',\n",
        "       'salarios reales':'RS', 'Indicador de Seguimiento a la Economía':'EMI',\n",
        "       'imporenta real':'RIT', 'iva real':'RVT',\n",
        "       'Tasa de intervención de política monetaria (%)':'MPIR','TD':'UR'},inplace=True) #'TD-1':'UR-1','TD-2':'UR-2','TD-3':'UR-3','TD':'UR'}, inplace=True)\n",
        "\n",
        "\n",
        "X_new=Xdata.loc[1:,['GPR','RS','EMI','RIT','RVT','MPIR','UR']]#,\n",
        "Xdata=Xdata.iloc[:-1]\n",
        "Xdata[['GPR-1','RS-1','EMI-1','RIT-1','RVT-1','MPIR-1','UR-1']]=X_new.values#'GPR-1','RS-1','EMI-1','RIT-1','RVT-1','MPIR-1',\n",
        "\n",
        "#Xdata=pd.DataFrame(scaler.fit_transform(Xdata),columns=Xdata.columns)\n",
        "X_new=Xdata.loc[1:,['GPR-1','RS-1','EMI-1','RIT-1','RVT-1','MPIR-1','UR-1']]#'GPR','RS','EMI','RIT','RVT','MPIR',\n",
        "Xdata=Xdata.iloc[:-1]\n",
        "Xdata[['GPR-2','RS-2','EMI-2','RIT-2','RVT-2','MPIR-2','UR-2']]=X_new.values#'GPR-1','RS-1','EMI-1','RIT-1','RVT-1','MPIR-1',\n",
        "\n",
        "#Xdata=pd.DataFrame(scaler.fit_transform(Xdata),columns=Xdata.columns)\n",
        "X_new=Xdata.loc[1:,['GPR-2','RS-2','EMI-2','RIT-2','RVT-2','MPIR-2','UR-2']]#'GPR','RS','EMI','RIT','RVT','MPIR',\n",
        "Xdata=Xdata.iloc[:-1]\n",
        "Xdata[['GPR-3','RS-3','EMI-3','RIT-3','RVT-3','MPIR-3','UR-3']]=X_new.values#'GPR-1','RS-1','EMI-1','RIT-1','RVT-1','MPIR-1',\n",
        "\n",
        "#Xdata=pd.DataFrame(scaler.fit_transform(Xdata),columns=Xdata.columns)\n",
        "X_new=Xdata.loc[1:,['GPR-3','RS-3','EMI-3','RIT-3','RVT-3','MPIR-3','UR-3']]#'GPR','RS','EMI','RIT','RVT','MPIR',\n",
        "Xdata=Xdata.iloc[:-1]\n",
        "Xdata[['GPR-4','RS-4','EMI-4','RIT-4','RVT-4','MPIR-4','UR-4']]=X_new.values#'GPR-1','RS-1','EMI-1','RIT-1','RVT-1','MPIR-1',\n",
        "#Xdata=pd.DataFrame(scaler.fit_transform(Xdata),columns=Xdata.columns)\n",
        "X_new=Xdata.loc[1:,['GPR-4','RS-4','EMI-4','RIT-4','RVT-4','MPIR-4','UR-4']]#'GPR','RS','EMI','RIT','RVT','MPIR',\n",
        "Xdata=Xdata.iloc[:-1]\n",
        "Xdata[['GPR-5','RS-5','EMI-5','RIT-5','RVT-5','MPIR-5','UR-5']]=X_new.values#'GPR-1','RS-1','EMI-1','RIT-1','RVT-1','MPIR-1',\n",
        "# X_new=Xdata.loc[1:,'TD-1']\n",
        "# Xdata=Xdata.iloc[:-1]\n",
        "# Xdata['TD-2']=X_new.values\n",
        "# X_new=Xdata.loc[1:,'TD-2']\n",
        "# Xdata=Xdata.iloc[:-1]\n",
        "# Xdata['TD-3']=X_new.values\n",
        "print(Xdata.info())\n",
        "scaler= MinMaxScaler()\n",
        "Xdata.iloc[:,2:]=scaler.fit_transform(Xdata.iloc[:,2:])\n",
        "X=Xdata.drop(['UR-5','Year','Month'], axis=1).values\n",
        "y=Xdata['UR-5'].values\n",
        "\n",
        "Xdata\n",
        "i=['KRRBF','KNN','GP','Linear','Lasso','Elastic','SGD','BayesRid','RF','SVR']\n",
        "Xdata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6fC7BSCxkbs"
      },
      "source": [
        "# Time series cross validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqU5KgWUEVP0"
      },
      "source": [
        "##Classic Economic Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVBLbdXzEcOr"
      },
      "outputs": [],
      "source": [
        "# Evaluar el modelo ARIMA\n",
        "results_arima = time_series_cv(X,y, model_type='ARIMA',s=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30oCVw-Ojk6K"
      },
      "outputs": [],
      "source": [
        "# Evaluar el modelo SARIMA\n",
        "results_sarima = time_series_cv(X,y, model_type='SARIMA',s=0,seasonal_order=(3, 3, 3, 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_NVM0r7OQDD"
      },
      "source": [
        "##Machine Learning Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udV3oAyRNg_D"
      },
      "outputs": [],
      "source": [
        "results_KRRBF=time_series_cv(X, y,model_type=i[0],s=0,Na='s0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ElBpZLMlNg1J"
      },
      "outputs": [],
      "source": [
        "results_KNN=time_series_cv(X, y,model_type=i[1],s=0,Na='s0')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_GP=time_series_cv(X, y,model_type=i[2],s=0,Na='s04')"
      ],
      "metadata": {
        "id": "mp0NJ-MYzI-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JznZ5k4KNgjM"
      },
      "outputs": [],
      "source": [
        "results_Linear=time_series_cv(X, y,model_type=i[3],s=0,Na='s01')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZH619MS6NgY8"
      },
      "outputs": [],
      "source": [
        "results_Lasso=time_series_cv(X, y,model_type=i[4],s=0,Na='s03')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnL2fjRONgRb"
      },
      "outputs": [],
      "source": [
        "##########################\n",
        "results_Elastic=time_series_cv(X, y,model_type=i[5],s=0,Na='s01')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLTLgjXDNgJU"
      },
      "outputs": [],
      "source": [
        "results_SGD=time_series_cv(X, y,model_type=i[6],s=0,Na='s0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9gPDNzVNgAE"
      },
      "outputs": [],
      "source": [
        "results_BayesRid=time_series_cv(X, y,model_type=i[7],s=0,Na='s0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqXnIDg7NwOF"
      },
      "outputs": [],
      "source": [
        "results_RF=time_series_cv(X, y,model_type=i[8],s=0,Na='s01')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RsLJVTFsNy-5"
      },
      "outputs": [],
      "source": [
        "results_SVR=time_series_cv(X, y,model_type=i[9],s=0,Na='s0')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAeKO_96x1fp"
      },
      "source": [
        "# Sliding Window Cross-Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19GoDIVOx-zb"
      },
      "source": [
        "##Classic Economic Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rU993CexG-ss"
      },
      "outputs": [],
      "source": [
        "# Evaluar el modelo ARIMA\n",
        "results_arima_s = time_series_cv(X,y, model_type='ARIMA',s=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5TF0uQFG_00"
      },
      "outputs": [],
      "source": [
        "# Evaluar el modelo SARIMA\n",
        "results_sarima_s = time_series_cv(X,y, model_type='SARIMA',s=1,seasonal_order=(3, 3, 3, 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsElD1WEyNWU"
      },
      "source": [
        "##Machine Learning Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4Hve5r5M-sb"
      },
      "outputs": [],
      "source": [
        "results_KRRBF_s=time_series_cv(X, y,model_type=i[0],s=1,Na='s1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZM6H7YRPC39"
      },
      "outputs": [],
      "source": [
        "results_KNN_s=time_series_cv(X, y,model_type=i[1],s=1,Na='s1')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DPgUCDRIDSCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYuJlSkyQaiG"
      },
      "outputs": [],
      "source": [
        "results_GP_s=time_series_cv(X, y,model_type=i[2],s=1,Na='s12')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mxqy0N3oRZlW"
      },
      "outputs": [],
      "source": [
        "results_Linear_s=time_series_cv(X, y,model_type=i[3],s=1,Na='s1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPQI0_BKVWDV"
      },
      "outputs": [],
      "source": [
        "results_Lasso_s=time_series_cv(X, y,model_type=i[4],s=1,Na='s12')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lKKhxk0VwNU"
      },
      "outputs": [],
      "source": [
        "results_Elastic_s=time_series_cv(X, y,model_type=i[5],s=1,Na='s19')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ssb-LTLbV7ZD"
      },
      "outputs": [],
      "source": [
        "results_SGD_s=time_series_cv(X, y,model_type=i[6],s=1,Na='s1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PrAng89UWIEt"
      },
      "outputs": [],
      "source": [
        "results_BayesRid_s=time_series_cv(X, y,model_type=i[7],s=1,Na='s1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v6QwJIgpWR4z"
      },
      "outputs": [],
      "source": [
        "results_RF_s=time_series_cv(X, y,model_type=i[8],s=1,Na='s11')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p53x7y_gZfik"
      },
      "outputs": [],
      "source": [
        "results_SVR_s=time_series_cv(X, y,model_type=i[9],s=1,Na='s1')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "yxJaI2hOYFNP",
        "CGGIGYweOG-N",
        "U6fC7BSCxkbs",
        "mqU5KgWUEVP0",
        "9_NVM0r7OQDD",
        "JAeKO_96x1fp",
        "19GoDIVOx-zb",
        "IsElD1WEyNWU"
      ],
      "mount_file_id": "1kDI7RWZPImfT79DRTFA25iMC5K-pbRcG",
      "authorship_tag": "ABX9TyOXFHwcegWUE+9j9SJXhiTi",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}